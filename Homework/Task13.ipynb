{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b34c1a",
   "metadata": {},
   "source": [
    "Сравнить LSTM, RNN и GRU на задаче предсказания части речи (качество предсказания, скорость обучения, время инференса модели)  \n",
    " *к первой зачаче добавить bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "902e102d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T16:49:57.253930Z",
     "start_time": "2022-11-28T16:49:57.238341Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from time import perf_counter\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "039e6871",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T16:50:01.465404Z",
     "start_time": "2022-11-28T16:50:01.435459Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class DatasetSeq(Dataset):\n",
    "\n",
    "    def __init__(self, data_dir='./', train_lang='en'):\n",
    "        # open file\n",
    "        with open(data_dir + train_lang + '.train', 'r',\n",
    "                  encoding='utf-8') as f:\n",
    "            train = f.read().split('\\n\\n')\n",
    "\n",
    "        # delete extra tag markup\n",
    "        train = [x for x in train if not '_ ' in x]\n",
    "        # init vocabs of tokens for encoding { token:  id}\n",
    "        self.target_vocab = {}  # {NOUN: 1, VERB: 2, ADP: 3, NOUN: 1, PUNCT: 4}\n",
    "        self.word_vocab = {}  # {cat: 1, sat: 2, on: 3, mat: 4, '.': 5}\n",
    "        self.char_vocab = {}  # {c: 1, a: 2, t: 3, ' ': 4, s: 5}\n",
    "\n",
    "        # init encoded sequences lists (processed data)\n",
    "        self.encoded_sequences = []\n",
    "        self.encoded_targets = []\n",
    "        self.encoded_char_sequences = []\n",
    "        # n=1 because first value is padding\n",
    "        n_word = 1\n",
    "        n_target = 1\n",
    "        n_char = 1\n",
    "        for line in train:\n",
    "            sequence = []\n",
    "            target = []\n",
    "            chars = []\n",
    "            for item in line.split('\\n'):\n",
    "                if item != '':\n",
    "                    word, label = item.split(' ')\n",
    "\n",
    "                    if self.word_vocab.get(word) is None:\n",
    "                        self.word_vocab[word] = n_word\n",
    "                        n_word += 1\n",
    "                    if self.target_vocab.get(label) is None:\n",
    "                        self.target_vocab[label] = n_target\n",
    "                        n_target += 1\n",
    "                    for char in word:\n",
    "                        if self.char_vocab.get(char) is None:\n",
    "                            self.char_vocab[char] = n_char\n",
    "                            n_char += 1\n",
    "                    sequence.append(self.word_vocab[word])\n",
    "                    target.append(self.target_vocab[label])\n",
    "                    chars.append([self.char_vocab[char] for char in word])\n",
    "            self.encoded_sequences.append(sequence)  # n_seq x words_in_seq\n",
    "            self.encoded_targets.append(target)  # n_seq x words_in_seq\n",
    "            # n_seq x words_in_seq x word_len\n",
    "            self.encoded_char_sequences.append(chars)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            'data': self.encoded_sequences[index],  # words_in_seq\n",
    "            'char': self.encoded_char_sequences[index],  # words_in_seq\n",
    "            'target': self.encoded_targets[index],  # words_in_seq x word_len\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13fd589d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T16:50:27.098301Z",
     "start_time": "2022-11-28T16:50:24.320686Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = DatasetSeq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c69d7aec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T16:51:28.309604Z",
     "start_time": "2022-11-28T16:51:28.285794Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(input_data):\n",
    "    data = []\n",
    "    chars = []\n",
    "    targets = []\n",
    "    data_len = len(input_data)\n",
    "    max_len = 0\n",
    "    for item in input_data:\n",
    "        if len(item['data']) > max_len:\n",
    "            max_len = len(item['data'])\n",
    "        data.append(torch.as_tensor(item['data']))\n",
    "        chars.append(item['char'])\n",
    "        targets.append(torch.as_tensor(item['target']))\n",
    "    chars_seq = [[torch.as_tensor([0]) for _ in range(data_len)]\n",
    "                 for _ in range(max_len)]  # max_seq_len x batch_len\n",
    "    for j in range(data_len):  # batch_len\n",
    "        i = 0\n",
    "        while i < len(chars[j]):  # max_seq_len\n",
    "            # batch_len x seq_len x word_len\n",
    "            chars_seq[i][j] = torch.as_tensor(chars[j][i])\n",
    "            i += 1\n",
    "    for i in range(max_len):\n",
    "        chars_seq[i] = pad_sequence(chars_seq[i],\n",
    "                                    batch_first=True,\n",
    "                                    padding_value=0)\n",
    "    data = pad_sequence(data, batch_first=True, padding_value=0)\n",
    "    targets = pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "    return {'data': data, 'chars': chars_seq, 'target': targets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3124d852",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-28T16:54:40.077153Z",
     "start_time": "2022-11-28T16:54:40.056347Z"
    }
   },
   "outputs": [],
   "source": [
    "class SelectItem(nn.Module):\n",
    "\n",
    "    def __init__(self, item_index):\n",
    "        super().__init__()\n",
    "        self._name = 'selectitem'\n",
    "        self.item_index = item_index\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return inputs[self.item_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3f67629",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-29T06:25:36.410972Z",
     "start_time": "2022-11-29T06:25:36.402831Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, n_classes, arch=nn.RNN, do_r=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential()\n",
    "        self.seq.append(nn.Embedding(vocab_size, emb_dim))\n",
    "        self.seq.append(arch(emb_dim, hid_dim, batch_first=True))\n",
    "        self.seq.append(SelectItem(0))\n",
    "        self.seq.append(nn.Dropout(do_r))\n",
    "        self.seq.append(nn.Linear(hid_dim, n_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c2e5bdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-29T17:10:34.834907Z",
     "start_time": "2022-11-29T17:10:34.820696Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, data_dict in enumerate(dataloader):\n",
    "        words = data_dict['data'].to(device)\n",
    "        targets = data_dict['target'].to(device).view(-1)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(words).view(-1, n_classes)\n",
    "        loss = loss_fn(pred, targets)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(words)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "30d004aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-29T17:15:54.762208Z",
     "start_time": "2022-11-29T17:15:54.753958Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyper params\n",
    "VOCAB_SIZE = len(dataset.word_vocab) + 1\n",
    "N_CLASSES = len(dataset.target_vocab) + 1\n",
    "N_CHARS = len(dataset.char_vocab) + 1\n",
    "EMB_DIM = 300\n",
    "HID_DIM = 300\n",
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "SEED = 123\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "21bc4e9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-29T18:19:30.697764Z",
     "start_time": "2022-11-29T18:19:30.484930Z"
    }
   },
   "outputs": [],
   "source": [
    "simple_rnn = CustomRNN(VOCAB_SIZE, EMB_DIM, HID_DIM, N_CLASSES, arch=nn.RNN)\n",
    "gru_rnn = CustomRNN(VOCAB_SIZE, EMB_DIM, HID_DIM, N_CLASSES, arch=nn.GRU)\n",
    "lstm_rnn = CustomRNN(VOCAB_SIZE, EMB_DIM, HID_DIM, N_CLASSES, arch=nn.LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1463dbe1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-29T18:19:31.636844Z",
     "start_time": "2022-11-29T18:19:31.619390Z"
    }
   },
   "outputs": [],
   "source": [
    "simple_rnn_opt = Adam(simple_rnn.parameters())\n",
    "gru_rnn_opt = Adam(gru_rnn.parameters())\n",
    "lstm_rnn_opt = Adam(lstm_rnn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "59017dc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-29T18:19:32.600835Z",
     "start_time": "2022-11-29T18:19:32.595890Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a7514402",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-29T18:27:43.973989Z",
     "start_time": "2022-11-29T18:19:35.754388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.840596  [    0/21235]\n",
      "loss: 0.202978  [12800/21235]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.200353  [    0/21235]\n",
      "loss: 0.132252  [12800/21235]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.176207  [    0/21235]\n",
      "loss: 0.120550  [12800/21235]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.139270  [    0/21235]\n",
      "loss: 0.099176  [12800/21235]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.075701  [    0/21235]\n",
      "loss: 0.090879  [12800/21235]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.067742  [    0/21235]\n",
      "loss: 0.056539  [12800/21235]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.046602  [    0/21235]\n",
      "loss: 0.061081  [12800/21235]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.040490  [    0/21235]\n",
      "loss: 0.049335  [12800/21235]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.032992  [    0/21235]\n",
      "loss: 0.027958  [12800/21235]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.038102  [    0/21235]\n",
      "loss: 0.035124  [12800/21235]\n",
      "Done! Train time: 488.2036s\n"
     ]
    }
   ],
   "source": [
    "# Simple RNN model\n",
    "torch.manual_seed(SEED)\n",
    "start = perf_counter()\n",
    "for e in range(N_EPOCHS):\n",
    "    print(f\"Epoch {e+1}\\n-------------------------------\")\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    train(dataloader, simple_rnn, loss_fn, simple_rnn_opt)\n",
    "end = perf_counter()\n",
    "print(f'Done! Train time: {end - start:.4f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a9c16812",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-29T18:51:05.023770Z",
     "start_time": "2022-11-29T18:29:26.658140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.242490  [    0/21235]\n",
      "loss: 0.207586  [12800/21235]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.191505  [    0/21235]\n",
      "loss: 0.121904  [12800/21235]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.162709  [    0/21235]\n",
      "loss: 0.107539  [12800/21235]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.120855  [    0/21235]\n",
      "loss: 0.092994  [12800/21235]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.061491  [    0/21235]\n",
      "loss: 0.079759  [12800/21235]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.055441  [    0/21235]\n",
      "loss: 0.049521  [12800/21235]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.039002  [    0/21235]\n",
      "loss: 0.052963  [12800/21235]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.030167  [    0/21235]\n",
      "loss: 0.038532  [12800/21235]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.024477  [    0/21235]\n",
      "loss: 0.022229  [12800/21235]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.029689  [    0/21235]\n",
      "loss: 0.025807  [12800/21235]\n",
      "Done! Train time: 1298.3559s\n"
     ]
    }
   ],
   "source": [
    "# GRU RNN model\n",
    "torch.manual_seed(SEED)\n",
    "start = perf_counter()\n",
    "for e in range(N_EPOCHS):\n",
    "    print(f\"Epoch {e+1}\\n-------------------------------\")\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    train(dataloader, gru_rnn, loss_fn, gru_rnn_opt)\n",
    "end = perf_counter()\n",
    "print(f'Done! Train time: {end - start:.4f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5769e8da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-29T19:19:07.498861Z",
     "start_time": "2022-11-29T18:52:11.487144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.796016  [    0/21235]\n",
      "loss: 0.226620  [12800/21235]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.201842  [    0/21235]\n",
      "loss: 0.123999  [12800/21235]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.170860  [    0/21235]\n",
      "loss: 0.110687  [12800/21235]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.122155  [    0/21235]\n",
      "loss: 0.091878  [12800/21235]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.065292  [    0/21235]\n",
      "loss: 0.083195  [12800/21235]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.058239  [    0/21235]\n",
      "loss: 0.049416  [12800/21235]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.039523  [    0/21235]\n",
      "loss: 0.049778  [12800/21235]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.031447  [    0/21235]\n",
      "loss: 0.041792  [12800/21235]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.025541  [    0/21235]\n",
      "loss: 0.022387  [12800/21235]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.024966  [    0/21235]\n",
      "loss: 0.024645  [12800/21235]\n",
      "Done! Train time: 1615.9966s\n"
     ]
    }
   ],
   "source": [
    "# LSTM RNN model\n",
    "torch.manual_seed(SEED)\n",
    "start = perf_counter()\n",
    "for e in range(N_EPOCHS):\n",
    "    print(f\"Epoch {e+1}\\n-------------------------------\")\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    train(dataloader, lstm_rnn, loss_fn, lstm_rnn_opt)\n",
    "end = perf_counter()\n",
    "print(f'Done! Train time: {end - start:.4f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d293a796",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-29T19:25:28.242509Z",
     "start_time": "2022-11-29T19:25:28.003510Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(simple_rnn.state_dict(), 'simple_rnn.pth')\n",
    "torch.save(gru_rnn.state_dict(), 'gru_rnn.pth')\n",
    "torch.save(lstm_rnn.state_dict(), 'lstm_rnn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0ef3f695",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-29T19:46:56.518941Z",
     "start_time": "2022-11-29T19:46:56.511909Z"
    }
   },
   "outputs": [],
   "source": [
    "phrase = 'Paul looked at her , caught by the odd savagery beneath her casual attitude'\n",
    "words = phrase.split(' ')\n",
    "tokens = [dataset.word_vocab[w] for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "56d7af6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-29T19:47:03.591005Z",
     "start_time": "2022-11-29T19:47:03.576005Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple RNN\n",
      "Inference time: 0.0036s\n",
      "['PROPN', 'VERB', 'ADP', 'PRON', 'PUNCT', 'VERB', 'ADP', 'DET', 'ADJ', 'NOUN', 'ADJ', 'PRON', 'ADJ', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "start = perf_counter()\n",
    "with torch.no_grad():\n",
    "    pred = simple_rnn(torch.tensor(tokens).unsqueeze(0).to(device))\n",
    "    labels = torch.argmax(pred, dim=-1).squeeze().cpu().detach().tolist()\n",
    "    end = perf_counter()\n",
    "\n",
    "print('Simple RNN')\n",
    "print(f'Inference time: {end - start:.4f}s')\n",
    "target_labels = list(dataset.target_vocab.keys())\n",
    "print([target_labels[l - 1] for l in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "19b5eb09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-29T19:47:07.538571Z",
     "start_time": "2022-11-29T19:47:07.513570Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU RNN\n",
      "Inference time: 0.0043s\n",
      "['PROPN', 'VERB', 'ADP', 'PRON', 'PUNCT', 'VERB', 'ADP', 'DET', 'ADJ', 'NOUN', 'ADP', 'PRON', 'ADJ', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "start = perf_counter()\n",
    "with torch.no_grad():\n",
    "    pred = gru_rnn(torch.tensor(tokens).unsqueeze(0).to(device))\n",
    "    labels = torch.argmax(pred, dim=-1).squeeze().cpu().detach().tolist()\n",
    "    end = perf_counter()\n",
    "\n",
    "print('GRU RNN')\n",
    "print(f'Inference time: {end - start:.4f}s')\n",
    "target_labels = list(dataset.target_vocab.keys())\n",
    "print([target_labels[l - 1] for l in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4136cc5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-29T19:47:47.399395Z",
     "start_time": "2022-11-29T19:47:47.377757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM RNN\n",
      "Inference time: 0.0044s\n",
      "['PROPN', 'VERB', 'ADP', 'PRON', 'PUNCT', 'VERB', 'ADP', 'DET', 'ADJ', 'PRON', 'NOUN', 'PRON', 'ADJ', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "start = perf_counter()\n",
    "with torch.no_grad():\n",
    "    pred = lstm_rnn(torch.tensor(tokens).unsqueeze(0).to(device))\n",
    "    labels = torch.argmax(pred, dim=-1).squeeze().cpu().detach().tolist()\n",
    "    end = perf_counter()\n",
    "\n",
    "print('LSTM RNN')\n",
    "print(f'Inference time: {end - start:.4f}s')\n",
    "target_labels = list(dataset.target_vocab.keys())\n",
    "print([target_labels[l - 1] for l in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2590a415",
   "metadata": {},
   "source": [
    "## Выводы:\n",
    "Как видим все модели неплохо справились, однако со словом beneath справилась только GRU.<br>\n",
    "GRU и LSTM по времени инференса почти не отличаются, обычная RNN работает чуть быстрее.<br>\n",
    "С точки зрения качества предсказания на обучающей выборке обычная RNN в 1.5 хуже по функции потерь, однако она обучилась примерно за 8 минут, против 22 минут у GRU и 27 минут у LSTM.<br>\n",
    "Если нет потребности в сильной точности, то можно ограничится простой RNN. В более сложных задачах наверное лучше использовать GRU и LSTM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
