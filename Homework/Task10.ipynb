{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cde8d76",
   "metadata": {},
   "source": [
    "1. Написать на PyTorch forward и backward полносвязного слоя без использования autograd\n",
    "1. Написать 1-2 адаптивных оптимизатора\n",
    "1. Решить задачу нахождения корней квадратного уравнения методом градиентного спуска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23cb4754",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T12:10:56.632789Z",
     "start_time": "2022-11-12T12:10:55.216819Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7685508a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T12:10:56.656066Z",
     "start_time": "2022-11-12T12:10:56.640825Z"
    },
    "code_folding": [
     0,
     3,
     7,
     10
    ]
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "def sigmoid_backward(da, x):\n",
    "    sig = sigmoid(x)\n",
    "    return da * sig * (1 - sig)\n",
    "\n",
    "def relu(x):\n",
    "    return torch.maximum(torch.zeros_like(x), x)\n",
    "\n",
    "def relu_backward(da, x):\n",
    "    da = torch.tensor(da)\n",
    "    da[x <= 0] = 0\n",
    "    return da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4af56c84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T12:10:56.711437Z",
     "start_time": "2022-11-12T12:10:56.662640Z"
    }
   },
   "outputs": [],
   "source": [
    "def mse_loss(t, y):\n",
    "    return torch.pow(t - y, 2)\n",
    "\n",
    "def d_mse_loss(t, y):\n",
    "    return 2 * (y - t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722f95a6",
   "metadata": {},
   "source": [
    "PyTorch forward и backward полносвязного слоя без использования autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45310701",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T12:10:56.736761Z",
     "start_time": "2022-11-12T12:10:56.715937Z"
    },
    "code_folding": [
     1,
     9
    ]
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n_inp=10, n_out=1, activation=None):\n",
    "        self.w = torch.rand(n_out, n_inp) * 0.1\n",
    "        self.b = torch.rand(n_out, 1) * 0.1\n",
    "        self.n_inp = n_inp\n",
    "        self.n_out = n_out\n",
    "        self.activ = activation\n",
    "        self._clear_state()\n",
    "\n",
    "    def _clear_state(self):\n",
    "        self.lin = None\n",
    "        self.inp = None\n",
    "        self.d_w = None\n",
    "        self.d_b = None\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        self.inp = input_\n",
    "        if type(self.w) != type(self.inp):\n",
    "            print(type(self.w), type(self.inp))\n",
    "        self.lin = self.w @ self.inp + self.b\n",
    "        out = self.activ(self.lin) if self.activ else self.lin\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        if self.activ == sigmoid:\n",
    "            grad_lin = sigmoid_backward(grad, self.lin) \n",
    "        elif self.activ == relu:\n",
    "            grad_lin = relu_backward(grad, self.lin)\n",
    "        else:\n",
    "            grad_lin = grad\n",
    "        \n",
    "        m = self.inp.size()[1]\n",
    "        self.d_w = grad_lin @ self.inp.t() / m\n",
    "        self.d_b = torch.sum(grad_lin, 1, keepdims=True) / m\n",
    "\n",
    "        grad = self.w.t() @ grad_lin\n",
    "\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ce7a324",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T12:10:56.758467Z",
     "start_time": "2022-11-12T12:10:56.742876Z"
    }
   },
   "outputs": [],
   "source": [
    "layer = Layer(1, 10, sigmoid)\n",
    "input_ = torch.tensor([[1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01d81157",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T12:10:56.771711Z",
     "start_time": "2022-11-12T12:10:56.761763Z"
    },
    "code_folding": [
     3,
     10,
     14
    ]
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, arch: tuple[tuple[int, int]], activation: Callable = None):\n",
    "        self.layers = []\n",
    "        for i, neurons in enumerate(arch):\n",
    "            self.layers.append(Layer(neurons[0], neurons[1], \n",
    "                                     activation=activation if i < len(arch) - 1 else None))\n",
    "        self._clear_state()\n",
    "        \n",
    "    def _clear_state(self):\n",
    "        for layer in self.layers:\n",
    "            layer._clear_state()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f91c68",
   "metadata": {},
   "source": [
    "Адаптивный оптимизатор RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a0dc07c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T12:10:56.786901Z",
     "start_time": "2022-11-12T12:10:56.775153Z"
    },
    "code_folding": [
     17
    ]
   },
   "outputs": [],
   "source": [
    "class RMSprop:\n",
    "    def __init__(self, model: Network, rho=0.9, lr=0.01):\n",
    "        self.model = model\n",
    "        self.rho = rho\n",
    "        self.lr = lr\n",
    "        self.accum = [[torch.zeros_like(layer.w),\n",
    "                       torch.zeros_like(layer.b)] for layer in model.layers]\n",
    "    \n",
    "    def step(self):\n",
    "        for i, layer in enumerate(self.model.layers):\n",
    "            self.accum[i][0] = self.rho * self.accum[i][0] + (1 - self.rho) * layer.d_w**2\n",
    "            self.accum[i][1] = self.rho * self.accum[i][1] + (1 - self.rho) * layer.d_b**2\n",
    "            adapt_lr_w = self.lr / torch.sqrt(self.accum[i][0])\n",
    "            adapt_lr_b = self.lr / torch.sqrt(self.accum[i][1])\n",
    "            layer.w -= adapt_lr_w * layer.d_w\n",
    "            layer.b -= adapt_lr_b * layer.d_b\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.model._clear_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83abc26d",
   "metadata": {},
   "source": [
    "Адаптивный оптимизатор Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93966173",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T12:10:56.802368Z",
     "start_time": "2022-11-12T12:10:56.790100Z"
    },
    "code_folding": [
     22
    ]
   },
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, model: Network, beta1=0.9, beta2=0.9, lr=0.01):\n",
    "        self.model = model\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.lr = lr\n",
    "        self.accum = [[torch.zeros_like(layer.w),\n",
    "                       torch.zeros_like(layer.b)] for layer in model.layers]\n",
    "        self.vel = [[torch.zeros_like(layer.w),\n",
    "                     torch.zeros_like(layer.b)] for layer in model.layers]\n",
    "    \n",
    "    def step(self):\n",
    "        for i, layer in enumerate(self.model.layers):\n",
    "            self.vel[i][0] = self.beta1 * self.vel[i][0] + (1 - self.beta1) * layer.d_w\n",
    "            self.vel[i][1] = self.beta1 * self.vel[i][1] + (1 - self.beta1) * layer.d_b\n",
    "            self.accum[i][0] = self.beta2 * self.accum[i][0] + (1 - self.beta2) * layer.d_w**2\n",
    "            self.accum[i][1] = self.beta2 * self.accum[i][1] + (1 - self.beta2) * layer.d_b**2\n",
    "            adapt_lr_w = self.lr / torch.sqrt(self.accum[i][0])\n",
    "            adapt_lr_b = self.lr / torch.sqrt(self.accum[i][1])\n",
    "            layer.w -= adapt_lr_w * self.vel[i][0]\n",
    "            layer.b -= adapt_lr_b * self.vel[i][1]\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.model._clear_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a34ca61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T12:10:56.814807Z",
     "start_time": "2022-11-12T12:10:56.806263Z"
    }
   },
   "outputs": [],
   "source": [
    "x = 4 * torch.rand(2000) - 2\n",
    "y = x**3 + np.random.randn() * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59120c09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T12:12:09.768895Z",
     "start_time": "2022-11-12T12:10:56.822487Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "RMSprop: tensor([[1.8781]]) tensor([[3.6419]]) tensor([[-1.6652]]) tensor([[-3.3061]])\n",
      "Adam:    tensor([[2.0852]]) tensor([[3.9322]]) tensor([[-1.6329]]) tensor([[-3.3339]])\n",
      "Epoch: 1\n",
      "RMSprop: tensor([[1.8387]]) tensor([[3.6209]]) tensor([[-1.6674]]) tensor([[-3.3144]])\n",
      "Adam:    tensor([[2.0851]]) tensor([[3.9861]]) tensor([[-1.6563]]) tensor([[-3.4038]])\n",
      "Epoch: 2\n",
      "RMSprop: tensor([[1.8114]]) tensor([[3.6093]]) tensor([[-1.6783]]) tensor([[-3.3183]])\n",
      "Adam:    tensor([[2.0495]]) tensor([[3.9272]]) tensor([[-1.7048]]) tensor([[-3.7805]])\n",
      "Epoch: 3\n",
      "RMSprop: tensor([[1.7918]]) tensor([[3.6078]]) tensor([[-1.6917]]) tensor([[-3.3236]])\n",
      "Adam:    tensor([[1.9666]]) tensor([[3.9643]]) tensor([[-1.7352]]) tensor([[-4.5526]])\n",
      "Epoch: 4\n",
      "RMSprop: tensor([[1.7772]]) tensor([[3.6136]]) tensor([[-1.7057]]) tensor([[-3.3303]])\n",
      "Adam:    tensor([[1.8475]]) tensor([[4.4827]]) tensor([[-1.6221]]) tensor([[-5.5462]])\n",
      "Epoch: 5\n",
      "RMSprop: tensor([[1.7660]]) tensor([[3.6249]]) tensor([[-1.7195]]) tensor([[-3.3376]])\n",
      "Adam:    tensor([[1.4498]]) tensor([[5.6877]]) tensor([[-1.1825]]) tensor([[-6.8964]])\n",
      "Epoch: 6\n",
      "RMSprop: tensor([[1.7570]]) tensor([[3.6405]]) tensor([[-1.7327]]) tensor([[-3.3452]])\n",
      "Adam:    tensor([[0.7509]]) tensor([[6.7382]]) tensor([[-1.0909]]) tensor([[-7.7671]])\n",
      "Epoch: 7\n",
      "RMSprop: tensor([[1.7496]]) tensor([[3.6606]]) tensor([[-1.7450]]) tensor([[-3.3527]])\n",
      "Adam:    tensor([[0.8626]]) tensor([[7.4232]]) tensor([[-1.0782]]) tensor([[-7.7110]])\n",
      "Epoch: 8\n",
      "RMSprop: tensor([[1.7431]]) tensor([[3.6880]]) tensor([[-1.7555]]) tensor([[-3.3589]])\n",
      "Adam:    tensor([[0.8268]]) tensor([[7.4115]]) tensor([[-1.1214]]) tensor([[-7.7994]])\n",
      "Epoch: 9\n",
      "RMSprop: tensor([[1.7369]]) tensor([[3.7361]]) tensor([[-1.7609]]) tensor([[-3.3600]])\n",
      "Adam:    tensor([[0.7863]]) tensor([[7.3373]]) tensor([[-1.1450]]) tensor([[-7.9390]])\n",
      "Epoch: 10\n",
      "RMSprop: tensor([[1.7262]]) tensor([[3.8580]]) tensor([[-1.7468]]) tensor([[-3.3509]])\n",
      "Adam:    tensor([[0.7792]]) tensor([[7.3334]]) tensor([[-1.1472]]) tensor([[-7.9056]])\n",
      "Epoch: 11\n",
      "RMSprop: tensor([[1.6812]]) tensor([[4.1369]]) tensor([[-1.6896]]) tensor([[-3.3635]])\n",
      "Adam:    tensor([[0.8686]]) tensor([[7.5737]]) tensor([[-1.1408]]) tensor([[-7.9372]])\n",
      "Epoch: 12\n",
      "RMSprop: tensor([[1.5425]]) tensor([[4.6180]]) tensor([[-1.5907]]) tensor([[-3.5309]])\n",
      "Adam:    tensor([[0.8602]]) tensor([[7.7806]]) tensor([[-1.1476]]) tensor([[-7.9391]])\n",
      "Epoch: 13\n",
      "RMSprop: tensor([[1.2824]]) tensor([[5.3584]]) tensor([[-1.4703]]) tensor([[-4.0863]])\n",
      "Adam:    tensor([[0.8811]]) tensor([[7.8178]]) tensor([[-1.1499]]) tensor([[-7.9740]])\n",
      "Epoch: 14\n",
      "RMSprop: tensor([[0.9588]]) tensor([[6.2881]]) tensor([[-1.3555]]) tensor([[-5.2023]])\n",
      "Adam:    tensor([[0.8792]]) tensor([[7.5985]]) tensor([[-1.1557]]) tensor([[-8.0400]])\n",
      "Epoch: 15\n",
      "RMSprop: tensor([[0.7237]]) tensor([[6.6432]]) tensor([[-1.2648]]) tensor([[-6.5404]])\n",
      "Adam:    tensor([[0.9041]]) tensor([[7.8838]]) tensor([[-1.1551]]) tensor([[-8.0423]])\n",
      "Epoch: 16\n",
      "RMSprop: tensor([[0.6824]]) tensor([[6.7223]]) tensor([[-1.2091]]) tensor([[-7.3665]])\n",
      "Adam:    tensor([[0.8431]]) tensor([[7.5041]]) tensor([[-1.1609]]) tensor([[-8.1252]])\n",
      "Epoch: 17\n",
      "RMSprop: tensor([[0.6789]]) tensor([[6.7650]]) tensor([[-1.1681]]) tensor([[-7.7593]])\n",
      "Adam:    tensor([[0.9167]]) tensor([[7.8095]]) tensor([[-1.1458]]) tensor([[-8.0434]])\n",
      "Epoch: 18\n",
      "RMSprop: tensor([[0.6807]]) tensor([[6.7681]]) tensor([[-1.1281]]) tensor([[-7.7423]])\n",
      "Adam:    tensor([[0.9583]]) tensor([[7.9351]]) tensor([[-1.1502]]) tensor([[-8.1201]])\n",
      "Epoch: 19\n",
      "RMSprop: tensor([[0.6903]]) tensor([[6.7953]]) tensor([[-1.1341]]) tensor([[-7.8231]])\n",
      "Adam:    tensor([[0.9192]]) tensor([[7.9175]]) tensor([[-1.1621]]) tensor([[-8.1397]])\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "model1 = Network(((1, 100), (100, 1)), activation=sigmoid)\n",
    "model2 = Network(((1, 100), (100, 1)), activation=sigmoid)\n",
    "rmsprop = RMSprop(model1)\n",
    "adam = Adam(model2)\n",
    "for e in range(epochs):\n",
    "    for i, (val, t) in enumerate(zip(x, y)):\n",
    "        rmsprop.zero_grad()\n",
    "        adam.zero_grad()\n",
    "        pred1 = model1.forward(torch.tensor([[val]]))\n",
    "        pred2 = model2.forward(torch.tensor([[val]]))\n",
    "        loss1 = mse_loss(t, pred1)\n",
    "        loss2 = mse_loss(t, pred2)\n",
    "        grad1 = d_mse_loss(t, pred1)\n",
    "        grad2 = d_mse_loss(t, pred2)\n",
    "        model1.backward(grad1)\n",
    "        model2.backward(grad2)\n",
    "        rmsprop.step()\n",
    "        adam.step()\n",
    "        \n",
    "    print(f'Epoch: {e}')\n",
    "    print('RMSprop:', model1.forward(torch.tensor([[1.]])), model1.forward(torch.tensor([[2.]])),\n",
    "          model1.forward(torch.tensor([[-1.]])), model1.forward(torch.tensor([[-2.]])))\n",
    "    print('Adam:   ', model2.forward(torch.tensor([[1.]])), model2.forward(torch.tensor([[2.]])),\n",
    "          model2.forward(torch.tensor([[-1.]])), model2.forward(torch.tensor([[-2.]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7387dad",
   "metadata": {},
   "source": [
    "Решить задачу нахождения корней квадратного уравнения методом градиентного спуска:  \n",
    "$ax^2 + bx + c = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1711da8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T12:12:09.791529Z",
     "start_time": "2022-11-12T12:12:09.775880Z"
    }
   },
   "outputs": [],
   "source": [
    "a, b, c = 1, 2, -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5f7adc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T12:12:09.845770Z",
     "start_time": "2022-11-12T12:12:09.803038Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88216c20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T12:12:09.884244Z",
     "start_time": "2022-11-12T12:12:09.849458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root is -3.0000, loss is  0.0000\n",
      "Root is  1.0000, loss is  0.0000\n",
      "Root is -3.0000, loss is  0.0000\n",
      "Root is -3.0000, loss is  0.0000\n",
      "Root is  1.0000, loss is  0.0000\n",
      "Root is  1.0000, loss is  0.0000\n",
      "Root is  1.0000, loss is  0.0000\n",
      "Root is  1.0000, loss is  0.0000\n",
      "Root is  1.0000, loss is  0.0000\n",
      "Root is -3.0000, loss is  0.0000\n"
     ]
    }
   ],
   "source": [
    "def forward(x):\n",
    "    return a * x**2 + b * x + c\n",
    "\n",
    "def backward(x, grad):\n",
    "    return (2 * a * x + b) * grad\n",
    "\n",
    "def loss(pred, t):\n",
    "    return (pred - t)**2\n",
    "\n",
    "def grad_loss(pred, t):\n",
    "    return 2 * (pred - t)\n",
    "\n",
    "lr = 0.001\n",
    "for _ in range(10):\n",
    "    x = 10 * random() - 5\n",
    "    for i in range(1000):\n",
    "        pred = forward(x)\n",
    "        grad = backward(x, grad_loss(pred, 0))\n",
    "        x -= grad * lr\n",
    "    print(f'Root is {x:>7.4f}, loss is {loss(forward(x), 0):>7.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
